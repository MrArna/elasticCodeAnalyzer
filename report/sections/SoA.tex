\section{Related work}
\label{sec:soa}
Several works in the literature propose different approaches to both performance maximization under a power cap and power consumption minimization under performance constraints. For instance, some of them exploit \gls{DVFS} techniques and try to pack together similar threads \cite{chocran2011pack}, while others try to minimize the times the cores go into idle states, in order to save the power spent in going from an idle state back to an active one \cite{kim2015racing}. Most of these works aims at reducing costs in data centers \cite{horvath2007dynamic,meisner2011power,shen2013power} or to increase battery life in power-constrained devices \cite{kim2013formal,mohapatra2005cross,ferroni2013mpower}, while our main focus is performance maximization under a strict power cap. 

A remarkable work with our same goal is PUPiL, a framework that aims to minimize and to maximize respectively the concept of timeliness and efficiency: \emph{timeliness} is intended as the ability of the system in enforcing a new cap, while \emph{efficiency} is meant as the performance delivered by the applications under a fixed power cap \cite{zhang2016maximizing}. In order to achieve these goals, PUPiL exploits both hardware (i.e., the Intel \gls{RAPL} interface \cite{david2010rapl}) and software (i.e., resource partitioning and allocation) techniques inside a canonical \gls{ODA} control loop, one of the main building blocks of \emph{self-aware computing}.

Even though the approach proposed by PUPiL is effective, we identified two non-negligible limitations of the proposed solution: first, the applications running on the system need to be instrumented with the \emph{Heartbeat framework} \cite{hoffman2009application,hoffman2010application}, in order to provide a uniform metric of throughput to the decision phase; second, the tool is meant to work with applications running bare-metal on Linux. Both these conditions might not be met in the context of a multi-tenant virtualized environment, in which a virtualization layer allows the execution of multiple workloads and ensures isolation to each of them. This is the case of the \emph{Xen hypervisor} \cite{barham2002xen}, a bare-metal type-1 hypervisor widely adopted in real production environments \cite{productionExample}, that runs directly as an abstraction layer between the hardware and the hosted virtual machines, called \emph{domains} in the Xen terminology. It is based on a microkernel design, providing services that allow multiple operating systems to concurrently run on the same hardware. A privileged domain, called \emph{Dom0}, is in charge of managing the \emph{DomU} unprivileged domains. In this context, the high isolation of each tenant, seen as a \emph{black box}, makes any instrumentation of the code of the hosted applications not feasible in a real production environment.

In this paper, we want to extend the current implementation of PUPiL\footnote{All source code, scripts, inputs, and patches are available at: https://github.com/PUPiL2015/PUPIL.git} to make it work in a virtualized environment based on the Xen hypervisor, without requiring any instrumentation of the guest workloads, as discussed in the next sections.

% On 32-bit x86, the Xen host kernel code runs in Ring 0, while the hosted domains run in Ring 1 (kernel) and Ring 3 (applications).

% The hypervisor supports running two different types of guests: Paravirtualization (PV) and Full or Hardware assisted Virtualization (HVM) \fixme{cit. da dove hai preso queste info}.


